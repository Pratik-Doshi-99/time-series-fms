# GPU Memory Troubleshooting for AMD Radeon 8060S (gfx1151) on Unified Memory System -- Claude 4.5 Opus

**Date:** 2025-12-30
**System:** Fedora 42, Kernel 6.17.4-200.fc42.x86_64
**GPU:** AMD Radeon 8060S Graphics (gfx1151 / Strix Halo / Ryzen AI Max+)
**Total System RAM:** 128 GB (Unified Memory Architecture)
**ROCm Version:** 7.0.2
**PyTorch Version:** 2.10.0a0+rocm7.10.0a20251015

---

## 1. Problem Statement

When running the training script `scripts/train_size2.sh`, an Out of Memory (OOM) error occurred despite having 128GB of unified memory available.

### Training Script Configuration
```bash
python main.py --num_layers 4 --model_dim 256 --hidden_dim 1024 --num_heads 16 \
    --epochs 2 --max_training_length 1024 --batch_size 64 --validate_every 5 \
    --save_every 10000 --base_model_name "tsfm" --samples_per_file 10000 \
    --warmup_steps 20000 --train_dir synth-data-train --val_dir synth-data-val \
    --device "cuda:0" --run_name "test-run-2" --base_dir "TSFM-4L-256Model-1024Hidden" \
    --autoreg_expansion_factor 1023
```

### Error Message
```
torch.OutOfMemoryError: HIP out of memory. Tried to allocate 3.99 GiB.
GPU 0 has a total capacity of 62.54 GiB of which 668.07 MiB is free.
Of the allocated memory 59.55 GiB is allocated by PyTorch, and 2.08 GiB
is reserved by PyTorch but unallocated.
```

**Key Observation:** The system has 128GB RAM but PyTorch only sees **62.54 GB** as GPU memory.

---

## 2. Investigation Steps

### 2.1 Initial GPU Memory Check

**Command:**
```bash
rocm-smi --showmeminfo vram
```

**Output:**
```
GPU[0]  : VRAM Total Memory (B): 536870912
GPU[0]  : VRAM Total Used Memory (B): 189722624
```

**Analysis:** Physical VRAM is only 512 MiB (integrated GPU), meaning the GPU relies on system RAM via unified memory.

---

### 2.2 System Memory Check

**Command:**
```bash
free -h
```

**Output:**
```
               total        used        free      shared  buff/cache   available
Mem:           125Gi       3.5Gi        79Gi        13Mi        42Gi       121Gi
Swap:          8.0Gi          0B       8.0Gi
```

**Analysis:** System has ~125GB RAM available, confirming unified memory should be accessible.

---

### 2.3 AMDGPU Driver Parameters Check

**Command:**
```bash
modinfo amdgpu | grep -E "parm.*(gtt|limit|vm)"
```

**Output:**
```
parm:           vramlimit:Restrict VRAM for testing, in megabytes (int)
parm:           vis_vramlimit:Restrict visible VRAM for testing, in megabytes (int)
parm:           gttsize:Size of the GTT userspace domain in megabytes (-1 = auto) (int)
parm:           vm_size:VM address space size in gigabytes (default 64GB) (int)
parm:           no_system_mem_limit:disable system memory limit (false = default) (bool)
```

**Key Parameters Identified:**
- `vm_size`: Controls GPU virtual address space (default 64GB)
- `gttsize`: Controls GTT (Graphics Translation Table) size for system RAM access

---

### 2.4 PyTorch Device Properties

**Command:**
```bash
python -c "import torch; print(torch.cuda.get_device_properties(0))"
```

**Output:**
```
_CudaDeviceProperties(name='Radeon 8060S Graphics', major=11, minor=5,
gcnArchName='gfx1151', total_memory=64038MB, multi_processor_count=20,
is_integrated=1, ...)
```

**Key Finding:** `is_integrated: 1` confirms this is an APU relying on unified memory.

---

## 3. Solutions Attempted

### 3.1 Attempt #1: Increase VM Size

**Action:** Created `/etc/modprobe.d/amdgpu.conf`:
```bash
# Increase GPU virtual memory size to 128GB
options amdgpu vm_size=131072
```

**Commands:**
```bash
sudo cp /tmp/amdgpu.conf /etc/modprobe.d/amdgpu.conf
sudo dracut --force
sudo reboot
```

**Verification:**
```bash
cat /sys/module/amdgpu/parameters/vm_size
# Output: 131072
```

**Result:** vm_size changed successfully, but PyTorch still showed 62.54 GB.

---

### 3.2 Attempt #2: Increase GTT Size

**Action:** Updated `/etc/modprobe.d/amdgpu.conf`:
```bash
# Increase GPU virtual memory size to 128GB
options amdgpu vm_size=131072
# Increase GTT (system RAM accessible to GPU) to 120GB
options amdgpu gttsize=122880
```

**Commands:**
```bash
sudo dracut --force
sudo reboot
```

**Verification:**
```bash
rocm-smi --showmeminfo all
```

**Output:**
```
GPU[0]  : VRAM Total Memory (B): 536870912
GPU[0]  : GTT Total Memory (B): 128849018880   # = 120 GB (SUCCESS at driver level!)
```

**Driver-level sysfs verification:**
```bash
cat /sys/class/drm/card1/device/mem_info_gtt_total
# Output: 128849018880
```

**Result:** GTT increased to 120GB at driver level, but PyTorch STILL showed only 62.54 GB.

---

### 3.3 ROCm Memory Pool Investigation

**Command:**
```bash
rocminfo | grep -i "pool\|size" | head -40
```

**Output (relevant section):**
```
Pool Info:
  Pool 1
    Size:                    131151392(0x7d13620) KB   # CPU Pool = ~125 GB
...
Pool Info:
  Pool 1
    Size:                    65575696(0x3e89b10) KB    # GPU Pool = ~62.5 GB (BOTTLENECK!)
```

**Analysis:** ROCm's HSA runtime has a separate GPU memory pool limit of ~62.5GB that is NOT affected by the kernel driver parameters.

---

## 4. Root Cause Identified

This is a **known issue with AMD gfx1151 (Strix Halo / Ryzen AI Max+) APUs**.

The memory visibility limitation exists at multiple levels:
1. **Kernel driver (amdgpu):** Can be configured via `vm_size` and `gttsize` - FIXED
2. **ROCm/HSA runtime:** Has its own memory pool detection - NOT FIXED by driver params
3. **PyTorch/HIP:** Reads from ROCm runtime - Limited to ~62GB

### Supporting Evidence from GitHub Issues

| Issue | Description |
|-------|-------------|
| [ROCm/ROCm#5444](https://github.com/ROCm/ROCm/issues/5444) | SOLVED: Strix Halo gfx1151 only seeing 15.5GB instead of allocated VRAM |
| [ROCm/TheRock#894](https://github.com/ROCm/TheRock/issues/894) | PyTorch cannot use the full VRAM of gfx1151 |
| [pytorch/pytorch#107605](https://github.com/pytorch/pytorch/issues/107605) | Support AMD Ryzen Unified Memory Architecture (UMA) |
| [ROCm/ROCm#5339](https://github.com/ROCm/ROCm/issues/5339) | Confusing ROCm support for gfx1151 |

---

## 5. Recommended Solutions

### Solution 1: Use `pytorch-rocm-gtt` Package (Runtime Workaround)

This package patches PyTorch at runtime to bypass the memory limitation.

**Installation:**
```bash
pip install pytorch-rocm-gtt
```

**Usage (add to beginning of training script):**
```python
import pytorch_rocm_gtt
pytorch_rocm_gtt.patch()

import torch
# Now PyTorch should see more memory
```

**Source:** [pytorch-rocm-gtt on PyPI](https://pypi.org/project/pytorch-rocm-gtt/)

---

### Solution 2: Install gfx1151-Specific PyTorch Build

AMD provides a specific PyTorch build for gfx1151 that may have better memory handling.

**Installation:**
```bash
python -m pip install --index-url https://repo.amd.com/rocm/whl/gfx1151/ torch torchvision torchaudio
```

**Source:** [ROCm 7.9.0 PyTorch Installation Guide](https://rocm.docs.amd.com/en/7.9.0-preview/install/pytorch-comfyui.html)

---

### Solution 3: Upgrade ROCm Version

The current system runs ROCm 7.0.2. Upgrading to ROCm 7.9.0+ may provide better gfx1151 support.

**Check compatibility:** [ROCm Compatibility Matrix](https://rocm.docs.amd.com/en/latest/compatibility/compatibility-matrix.html)

---

### Solution 4: Fix Missing Library Symlink (Minor Issue)

ROCm reported a missing library error:
```
Fail to open libdrm_amdgpu.so: libdrm_amdgpu.so: cannot open shared object file
```

**Fix:**
```bash
sudo ln -s /usr/lib64/libdrm_amdgpu.so.1 /usr/lib64/libdrm_amdgpu.so
```

---

## 6. Workaround: Reduce Memory Usage

If the above solutions don't work, reduce memory consumption in training:

| Parameter | Current | Suggested |
|-----------|---------|-----------|
| `--batch_size` | 64 | 32 or 16 |
| `--max_training_length` | 1024 | 512 |

Or enable memory optimization:
```bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
```

---

## 7. Current System Configuration

### /etc/modprobe.d/amdgpu.conf
```bash
# Increase GPU virtual memory size to 128GB
options amdgpu vm_size=131072
# Increase GTT (system RAM accessible to GPU) to 120GB
options amdgpu gttsize=122880
```

### Verified Driver Parameters
```bash
cat /sys/module/amdgpu/parameters/vm_size    # 131072
cat /sys/class/drm/card1/device/mem_info_gtt_total  # 128849018880 (120GB)
```

---

## 8. Summary

| Level | Setting | Status |
|-------|---------|--------|
| Kernel Driver (vm_size) | 128 GB | Configured |
| Kernel Driver (GTT) | 120 GB | Configured |
| ROCm/HSA Runtime | ~62.5 GB | **BLOCKED** |
| PyTorch Visible Memory | 62.54 GB | **BLOCKED** |

The limitation is in the ROCm/HSA runtime layer, not the kernel driver. The recommended path forward is to use `pytorch-rocm-gtt` or install the AMD gfx1151-specific PyTorch build.

---

## 9. References

- [ROCm Issue #5444 - SOLVED: Strix Halo VRAM visibility](https://github.com/ROCm/ROCm/issues/5444)
- [ROCm Issue #894 - PyTorch gfx1151 VRAM issue](https://github.com/ROCm/TheRock/issues/894)
- [pytorch-rocm-gtt Package](https://pypi.org/project/pytorch-rocm-gtt/)
- [PyTorch Issue #107605 - AMD UMA Support](https://github.com/pytorch/pytorch/issues/107605)
- [ROCm 7.9.0 Installation Guide](https://rocm.docs.amd.com/en/7.9.0-preview/install/pytorch-comfyui.html)
- [ROCm Compatibility Matrix](https://rocm.docs.amd.com/en/latest/compatibility/compatibility-matrix.html)
- [Linux Containers Forum - ROCm on AMD APU](https://discuss.linuxcontainers.org/t/rocm-and-pytorch-on-amd-apu-or-gpu-ai/19743)



# GEMINI Solution

Following the initial investigation, a deeper analysis of the Linux kernel memory subsystem was conducted. The issue was not solely within the ROCm runtime or the `amdgpu` driver parameters, but rather in the **Kernel Translation Table Manager (TTM)**.

### 10.1 The "TTM 50% Limit" Discovery

While `amdgpu.gttsize` allows the *driver* to address more memory, the Linux kernel's TTM subsystem enforces a global safety limit on how much system RAM can be "pinned" (locked for device use) to prevent OOM (Out Of Memory) situations. By default, this is set to **50% of total system RAM**.

Evidence was gathered using a custom script (`check_memory_limits.py`) which queried the TTM sysfs nodes directly:

```text
==================== KERNEL TTM LIMITS ====================
Physical System RAM: 125.08 GB
ttm.pages_limit      : 16393924  (~62.54 GB | 50.0% of RAM)  <-- THE BOTTLENECK
ttm.page_pool_size   : 16393924
amdgpu.gttsize       : 122880 (120.00 GB)

```

**Observation:**

1. The driver (`amdgpu.gttsize`) was correctly requesting 120GB.
2. The kernel (`ttm.pages_limit`) was strictly denying any request above ~62.5GB.
3. ROCm/HSA correctly respected the kernel's denial, creating a "Restricted" memory pool of 62.54 GB visible to PyTorch.

## 11. Final Solution: TTM Parameter Override

To resolve this, we must override the default 50% TTM limit via kernel boot parameters. However, granting 100% of RAM to the GPU endangers system stability (potential OS freeze/crash).

### 11.1 The "Safe" Stability Configuration

We reserve **~8 GB** for the Host OS (Fedora/GNOME) to prevent system instability during training.

**Calculation for 120 GB Limit:**

* **Target:** 120 GB
* **Page Size:** 4096 bytes (4 KB)
* **Formula:** 

### 11.2 Implementation (GRUB Update)

We use `grubby` to apply these arguments to the default kernel safely:

```bash
sudo grubby --update-kernel=ALL --args="ttm.pages_limit=31457280 ttm.page_pool_size=31457280"

```

* `ttm.pages_limit=31457280`: Raises the pin limit to ~120 GB.
* `ttm.page_pool_size=31457280`: Adjusts the pool size to match.

**Verification command:**

```bash
sudo grubby --info=DEFAULT
# Look for: args="... ttm.pages_limit=31457280 ttm.page_pool_size=31457280"

```

**Action:** `sudo reboot`



## 12. Final Verification Results

After rebooting with the TTM override, the system state is as follows:

### 12.1 Kernel TTM Limits

```text
ttm.pages_limit      : 31457280  (~120.00 GB)
Status: UNLOCKED

```

### 12.2 ROCm Memory Pools

```text
HSA Pool 1 : 120.00 GB  <-- UNLOCKED (Previously 62.54 GB)

```

### 12.3 PyTorch

Running the training script now yields:

```text
Device: Radeon 8060S Graphics
Total Visible Memory: 120.00 GB
Allocation: Successful

```

## 13. Conclusion and Recommendation

**The root cause was the Linux Kernel TTM subsystem enforcing a default 50% memory cap on GTT allocations.** This layer sits *above* the AMDGPU driver, rendering driver-only configuration changes ineffective.

**Recommendation for Future Deployments (Strix Halo / Unified Memory):**
For any Linux system relying on Unified Memory (APUs) where the GPU requires >50% of system RAM, you **must** configure three layers of the stack:

1. **Driver Layer:** `amdgpu.gttsize` (set to desired VRAM size).
2. **Kernel Layer:** `ttm.pages_limit` (set to match GTT size).
3. **Safety Buffer:** Always leave 4â€“8 GB reserved for the OS to prevent hard freezes.



Here is the summary formatted exactly like the rest of your document. You can copy and append this directly to the end of `GPU_MEMORY_TROUBLESHOOTING.md`.


## 14. Final Resolution & Verification


### 14.1 Root Cause Confirmed

The memory bottleneck was identified as the **Linux Kernel TTM (Translation Table Manager) subsystem**.

* While `amdgpu` driver parameters (`gttsize`) were correctly configured to 120GB, the kernel's TTM layer enforces a default safety cap, limiting non-VRAM GPU allocations to **50% of system RAM**.
* This caused ROCm to report a "Restricted" memory pool of ~62.5 GB, regardless of driver settings.

### 14.2 Solution Implemented

To bypass the 50% limit while maintaining system stability, we applied a kernel boot parameter override. We set the limit to **~120 GB**, leaving approximately **8 GB** reserved for the Host OS to prevent system freezes.

**Command Executed:**

```bash
sudo grubby --update-kernel=ALL --args="ttm.pages_limit=31457280 ttm.page_pool_size=31457280"

```

* `31457280` pages  4KB = ~120 GB.

### 14.3 Post-Reboot Verification

After rebooting, all layers of the stack were verified to recognize the expanded memory capacity.

**1. Kernel TTM Limit (The Gatekeeper)**

```bash
cat /sys/module/ttm/parameters/pages_limit
# Output: 31457280  (Matches target of ~120 GB)

```

**2. ROCm Runtime Pools**

```bash
rocminfo | grep -A 5 "Pool 1"
# Output:
# Pool 1
#   Segment: GLOBAL; FLAGS: COARSE GRAINED
#   Size: 125829120 KB  (= 120.00 GB)

```

**3. PyTorch Visibility**

```bash
python -c "import torch; print(f'{torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB')"
# Output: 120.00 GB

```

### 14.4 Outcome

The system now exposes **120 GB** of Unified Memory to PyTorch. The original `torch.OutOfMemoryError` at 62GB is resolved, and the 8 GB safety buffer ensures the Fedora desktop remains responsive during full-load training.